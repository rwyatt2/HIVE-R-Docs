---
title: "Phase 8: Production Deployment"
sidebar_label: "Phase 8: Production Deployment"
---

**

**â±ï¸ Time**: 8-10 hours

**ğŸ’° Budget**: $10-15

**ğŸ¯ Goal**: HIVE-R running in production, serving real traffic

**ğŸ¤– AI Workers**: HIVE-R SRE + Builder + Data Analyst agents

**ğŸŒ Step 8.1: Production Environment Setup**

**Your Instruction** (in Cursor):

  

  

Consult the SRE agent.

  

CONTEXT:

We need production infrastructure that's reliable, scalable, and cost-effective.

  

TASK:

Recommend and document production architecture:

OPTION A: Single Server (MVP, &lt;100 users)

Â  - VPS (DigitalOcean, Linode, Vultr) - $20-40/month

Â  - Docker Compose for orchestration

Â  - Caddy for HTTPS + reverse proxy

Â  - Automated backups to S3

Â  - Cost: ~$50/month

OPTION B: Multi-Server (100-1K users)

Â  - Load balancer + 2-3 app servers

Â  - Managed PostgreSQL

Â  - Managed Redis

Â  - Docker Swarm or Kubernetes

Â  - Cost: ~$200-300/month

OPTION C: Cloud Managed (1K+ users)

Â  - Cloud Run / ECS Fargate (auto-scaling)

Â  - RDS PostgreSQL

Â  - ElastiCache Redis

Â  - CloudFront CDN

Â  - Cost: $500-1500/month (usage-based)

  

Based on your needs, recommend one and create:

Â  - Infrastructure diagram

Â  - Setup scripts (terraform or bash)

Â  - Environment configuration

Â  - DNS setup instructions

Â  - SSL certificate setup (Let's Encrypt)

  

DELIVERABLES:

- docs/deployment/production-architecture.md

- Infrastructure as Code: deploy/terraform/ or deploy/docker-swarm/

- Setup checklist: docs/deployment/production-checklist.md

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the architecture recommendation:

â˜ Matches your user scale and budget

â˜ Has backup and recovery plan

â˜ Monitoring integrated

â˜ SSL/HTTPS configured

â˜ Secrets managed securely (not in git)

**For Option A (Single Server - Recommended for MVP)**:

  

  

# On your production server (Ubuntu 22.04)

# Copy this setup script to server

  

#!/bin/bash

# deploy/scripts/production-setup.sh

  

# Install Docker

curl -fsSL https://get.docker.com | sh

sudo usermod -aG docker $USER

  

# Install Caddy (for HTTPS)

sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https

curl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/caddy-stable-archive-keyring.gpg] https://dl.cloudsmith.io/public/caddy/stable/deb/debian any-version main" | sudo tee /etc/apt/sources.list.d/caddy-stable.list

sudo apt update && sudo apt install -y caddy

  

# Create app directory

mkdir -p /opt/hive-r

cd /opt/hive-r

  

# Clone repo (or copy files)

git clone https://github.com/yourusername/HIVE-R.git .

  

# Setup environment

cp .env.example .env

nano .envÂ  # Fill in production values

  

# Start services

docker-compose -f docker-compose.prod.yml up -d

  

# Configure Caddy for HTTPS

cat > /etc/caddy/Caddyfile &lt;< 'EOF'

api.yourdomain.com &#123;

Â  Â  reverse_proxy localhost:3000

Â  Â  encode gzip

&#125;

EOF

  

sudo systemctl restart caddy

  

echo "âœ… Production setup complete!"

echo "Visit https://api.yourdomain.com/health/ready"

  

**Test production deployment**:

  

  

# On your local machine, deploy to production

./deploy/scripts/deploy-production.sh

  

# Wait for deployment

# Check health

curl https://api.yourdomain.com/health/ready

  

# Should return: &#123;"status":"healthy"&#125;

  

**âœ… Validation**:

â˜ Server accessible via HTTPS

â˜ SSL certificate valid (Let's Encrypt)

â˜ Health checks passing

â˜ All services running (docker ps)

â˜ Logs visible (docker logs)

**ğŸ“Š Log Cost**: ~$0.10 (30K tokens)

**ğŸ“Š Step 8.2: Production Monitoring Setup**

**Your Instruction** (in Cursor):

  

  

Consult the SRE agent.

  

CONTEXT:

Production is running. Now we need visibility into its health.

  

TASK:

Set up production monitoring:

Â  1. Application monitoring

Â Â  Â  - Sentry (error tracking) - already configured

Â Â  Â  - Prometheus + Grafana (metrics) - deploy to prod

Â Â  Â  - Health check monitoring (UptimeRobot, Pingdom)

Â  2. Infrastructure monitoring

Â Â  Â  - Server resources (CPU, RAM, disk)

Â Â  Â  - Docker container health

Â Â  Â  - Network traffic

Â  3. Business metrics

Â Â  Â  - Active users

Â Â  Â  - Request volume

Â Â  Â  - Cost per day

Â Â  Â  - Revenue (if applicable)

Â  4. Alerting

Â Â  Â  - Slack for warnings

Â Â  Â  - PagerDuty/Opsgenie for critical (optional)

Â Â  Â  - Email for daily digests

  

DELIVERABLES:

- Production monitoring stack (deployed)

- Dashboards for all key metrics

- Alert rules configured

- On-call runbook

- Monitoring access credentials (secure)

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review monitoring coverage:

â˜ Covers all critical services

â˜ Alerts are actionable (not noise)

â˜ Has escalation path

â˜ Dashboard accessible to team

**Deploy monitoring**:

  

  

# On production server

cd /opt/hive-r

  

# Update docker-compose.prod.yml to include monitoring

cat >> docker-compose.prod.yml &lt;< 'EOF'

Â  prometheus:

Â  Â  image: prom/prometheus:latest

Â  Â  volumes:

Â  Â  Â  - ./prometheus.yml:/etc/prometheus/prometheus.yml

Â  Â  Â  - prometheus-data:/prometheus

Â  Â  restart: unless-stopped

Â  grafana:

Â  Â  image: grafana/grafana:latest

Â  Â  volumes:

Â  Â  Â  - grafana-data:/var/lib/grafana

Â  Â  environment:

Â  Â  Â  - GF_SECURITY_ADMIN_PASSWORD=$&#123;GRAFANA_PASSWORD&#125;

Â  Â  Â  - GF_SERVER_DOMAIN=grafana.yourdomain.com

Â  Â  restart: unless-stopped

  

volumes:

Â  prometheus-data:

Â  grafana-data:

EOF

  

# Add to Caddy for public access

cat >> /etc/caddy/Caddyfile &lt;< 'EOF'

grafana.yourdomain.com &#123;

Â  Â  reverse_proxy grafana:3000

&#125;

EOF

  

# Restart

docker-compose -f docker-compose.prod.yml up -d

sudo systemctl restart caddy

  

**Setup external monitoring** (UptimeRobot):

1. Go to [uptimerobot.com](http://uptimerobot.com) (free tier)

2. Add HTTP monitor for [https://api.yourdomain.com/health/ready](https://api.yourdomain.com/health/ready)

3. Set check interval to 5 minutes

4. Add alert contacts (email, Slack)

**âœ… Validation**:

â˜ Grafana accessible at [grafana.yourdomain.com](http://grafana.yourdomain.com)

â˜ Dashboards showing live data

â˜ Alerts configured in UptimeRobot

â˜ Received test alert successfully

**ğŸ“Š Log Cost**: ~$0.08 (25K tokens)

**ğŸ” Step 8.3: Production Security Hardening**

**Your Instruction** (in Cursor):

  

  

Consult the Security agent.

  

CONTEXT:

Production must be secure against attacks.

  

TASK:

Harden production environment:

Â  1. Server hardening

Â Â  Â  - Firewall (UFW): Allow only 80, 443, 22 (SSH)

Â Â  Â  - SSH: Key-only auth, disable password, non-standard port

Â Â  Â  - Fail2ban: Block brute-force attempts

Â Â  Â  - Automatic security updates

Â  2. Application hardening

Â Â  Â  - Rate limiting (already implemented, verify active)

Â Â  Â  - WAF (Cloudflare free tier or ModSecurity)

Â Â  Â  - DDoS protection (Cloudflare)

Â Â  Â  - Security headers (CSP, HSTS, X-Frame-Options)

Â  3. Data protection

Â Â  Â  - Database encryption at rest

Â Â  Â  - SSL/TLS for all connections

Â Â  Â  - Secrets in environment variables (not in git)

Â Â  Â  - Regular backups (automated, tested)

Â  4. Access control

Â Â  Â  - Least privilege principle

Â Â  Â  - Separate staging/production credentials

Â Â  Â  - Audit logging (who accessed what)

Â Â  Â  - 2FA for admin accounts

  

DELIVERABLES:

- Server hardening script: deploy/scripts/harden-server.sh

- Security audit report

- Backup and restore procedures

- Access control documentation

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review security measures:

â˜ All unnecessary ports closed

â˜ SSH hardened (keys only, non-default port)

â˜ WAF active (even free Cloudflare helps)

â˜ Backups automated and tested

**Run hardening script**:

  

  

#!/bin/bash

# deploy/scripts/harden-server.sh

  

# Setup firewall

sudo ufw default deny incoming

sudo ufw default allow outgoing

sudo ufw allow 22/tcpÂ  Â  # SSH (change port later)

sudo ufw allow 80/tcpÂ  Â  # HTTP

sudo ufw allow 443/tcp Â  # HTTPS

sudo ufw enable

  

# Harden SSH

sudo sed -i 's/#Port 22/Port 2222/' /etc/ssh/sshd_config

sudo sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config

sudo systemctl restart sshd

  

# Install fail2ban

sudo apt install -y fail2ban

sudo systemctl enable fail2ban

  

# Automatic security updates

sudo apt install -y unattended-upgrades

sudo dpkg-reconfigure -plow unattended-upgrades

  

# Setup automated backups

cat > /etc/cron.daily/backup-hive &lt;< 'EOF'

#!/bin/bash

TIMESTAMP=$(date +%Y%m%d-%H%M%S)

tar -czf /backups/hive-$&#123;TIMESTAMP&#125;.tar.gz /opt/hive-r/data

aws s3 cp /backups/hive-$&#123;TIMESTAMP&#125;.tar.gz s3://your-backup-bucket/

find /backups -mtime +7 -delete

EOF

chmod +x /etc/cron.daily/backup-hive

  

echo "âœ… Server hardened"

  

**Test backups**:

  

  

# Trigger backup manually

sudo /etc/cron.daily/backup-hive

  

# Verify backup exists

ls -lh /backups/

  

# Test restore (on staging server)

# scp backup, extract, verify data intact

  

**âœ… Validation**:

â˜ Firewall active (only necessary ports open)

â˜ SSH hardened (test login with key)

â˜ Fail2ban active (check sudo fail2ban-client status)

â˜ Backups running (check cron logs)

â˜ Restore tested successfully

**ğŸ“Š Log Cost**: ~$0.08 (25K tokens)

**ğŸ“‹ Step 8.4: Create Production Runbook**

**Your Instruction** (in Cursor):

  

  

Consult the SRE and Tech Writer agents.

  

CONTEXT:

When production breaks at 3am, you need clear instructions.

  

TASK:

Create comprehensive production runbook:

SECTIONS:

Â  1. Architecture Overview

Â Â  Â  - Diagram of production setup

Â Â  Â  - What each component does

Â Â  Â  - How to access each service

Â  2. Common Operations

Â Â  Â  - Deploy new version

Â Â  Â  - Rollback to previous version

Â Â  Â  - Restart service

Â Â  Â  - Scale up/down

Â Â  Â  - View logs

Â Â  Â  - Access database

Â  3. Troubleshooting

Â Â  Â  - Service is down â†’ Check health endpoint, restart if needed

Â Â  Â  - High latency â†’ Check Grafana, identify bottleneck

Â Â  Â  - High costs â†’ Check cost dashboard, identify expensive queries

Â Â  Â  - Database is locked â†’ Kill long-running queries

Â Â  Â  - Disk is full â†’ Clean old logs, expand volume

Â  4. Incident Response

Â Â  Â  - Severity levels (Critical, High, Medium, Low)

Â Â  Â  - Response procedures

Â Â  Â  - Communication templates

Â Â  Â  - Post-mortem template

Â  5. Regular Maintenance

Â Â  Â  - Daily: Check health dashboard

Â Â  Â  - Weekly: Review costs, update dependencies

Â Â  Â  - Monthly: Review logs, backup test, security audit

Â Â  Â  - Quarterly: Disaster recovery drill

  

DELIVERABLES:

- docs/operations/production-runbook.md

- Quick reference card (one-page PDF)

- Incident response template

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the runbook:

â˜ Covers all likely scenarios

â˜ Commands are copy-pasteable

â˜ Has contact information (who to escalate to)

â˜ Clear and actionable

**Example runbook sections**:

  

  

# Production Runbook

  

## Emergency Contacts

- On-call: [Your phone]

- Backup: [Team member]

- Hosting: [Provider support]

  

## Quick Commands

  

### View Logs

```bash

# Application logs

docker logs -f hive-app --tail 100

  

# System logs

sudo journalctl -u docker -f

  

**Restart Service**

  

  

cd /opt/hive-r

docker-compose -f docker-compose.prod.yml restart app

  

**Rollback**

  

  

# Rollback to previous version

docker-compose -f docker-compose.prod.yml pull app:previous

docker-compose -f docker-compose.prod.yml up -d app

  

**Incident Response**

**Severity: Critical (Service Down)**

1. Check health endpoint: curl https://api.yourdomain.com/health/ready

2. If timeout, check server: ssh production-server

3. Check Docker: docker ps (all containers running?)

4. Restart if needed: docker-compose restart

5. Check logs for errors: docker logs hive-app

6. Post in #incidents Slack channel

7. If not resolved in 15 min, call backup engineer

**Severity: High (Performance Degraded)**

1. Open Grafana dashboard

2. Check latency: Is p95 >2s?

3. Check cost: Is burn rate >$50/hour?

4. If cost spike: Enable rate limiting stricter

5. If latency spike: Check for slow queries in database

6. Document findings in incident log

  

  

  

**âœ… Validation**:

- [ ] Runbook exists and is comprehensive

- [ ] Team has reviewed and understands it

- [ ] Commands tested (actually work)

- [ ] Contact information up to date

  

**ğŸ“Š Log Cost**: ~$0.10 (30K tokens)

  

---

  

### ğŸ¯ Step 8.5: Go-Live Checklist & Launch

  

**Your Instruction** (in Cursor):

  

  

Consult the SRE and Data Analyst agents.

CONTEXT:

We're ready to launch. Create final pre-launch checklist.

TASK:

Create comprehensive go-live checklist:

PRE-LAUNCH (1 week before):

â˜ All tests passing in CI

â˜ Security audit complete (no critical issues)

â˜ Performance tested (can handle 10x current traffic)

â˜ Backups automated and tested

â˜ Monitoring dashboards ready

â˜ Runbook documented

â˜ DNS configured (propagation takes 24-48h)

â˜ SSL certificates valid (Let's Encrypt or purchased)

â˜ Team trained on runbook

â˜ Incident response plan tested (tabletop exercise)

LAUNCH DAY:

â˜ Final staging test (full user journey)

â˜ Database migrated to production

â˜ Environment variables set correctly

â˜ Deploy production version

â˜ Health checks passing

â˜ Smoke tests passing

â˜ Monitoring active and alerting

â˜ Announce internally (team Slack)

â˜ Monitor closely for first 4 hours

POST-LAUNCH (first week):

â˜ Daily: Review metrics, logs, costs

â˜ Day 1: User feedback survey

â˜ Day 3: Performance review

â˜ Day 7: Cost review, optimize if needed

â˜ Week 1 retrospective

DELIVERABLES:

â€¢ Go-live checklist: docs/operations/go-live-checklist.md

â€¢ Launch announcement template

â€¢ Day 1/Week 1 review templates

  

  

  

**â¸ï¸ YOUR APPROVAL GATE**:

  

Review the checklist:

- [ ] Nothing critical missing

- [ ] Realistic timeline

- [ ] Clear ownership (who does each item)

- [ ] Rollback plan ready

  

**Execute go-live**:

```bash

# On launch day

cd /opt/hive-r

  

# 1. Final backup before launch

./deploy/scripts/backup-now.sh

  

# 2. Deploy production version

git pull origin main

docker-compose -f docker-compose.prod.yml pull

docker-compose -f docker-compose.prod.yml up -d

  

# 3. Run smoke tests

./scripts/smoke-test.sh

  

# 4. Monitor

watch -n 5 'curl -s https://api.yourdomain.com/health/ready | jq'

  

# 5. Check Grafana dashboard

# Open in browser: https://grafana.yourdomain.com

  

# 6. Announce

./scripts/announce-launch.shÂ  # Posts to Slack

  

**First 24 Hours Monitoring**:

â˜ Hour 1: Health checks green

â˜ Hour 4: No incidents

â˜ Hour 12: Costs tracking as expected

â˜ Hour 24: User feedback collected

**âœ… Validation**:

â˜ All checklist items complete

â˜ System running smoothly

â˜ No major incidents

â˜ Team confident in operations

**ğŸ“Š Log Cost**: ~$0.05 (15K tokens)

**ğŸ‰ Step 8.6: Post-Launch Optimization & Retrospective**

**Your Instruction** (in Cursor):

  

  

Consult the Data Analyst and Tech Writer agents.

  

CONTEXT:

You've been in production for 1 week. Time to review and optimize.

  

TASK:

Conduct Week 1 retrospective:

DATA ANALYSIS:

Â  - Actual vs predicted costs

Â  - Actual vs predicted traffic

Â  - Error rate trends

Â  - Performance trends (latency, throughput)

Â  - User behavior patterns

Â  - Cache hit rates

Â  - Model routing effectiveness

  

FINDINGS:

Â  - What worked well?

Â  - What didn't work as expected?

Â  - What surprised us?

Â  - Where are the bottlenecks?

Â  - Where can we optimize costs?

  

OPTIMIZATION PLAN:

Â  - Quick wins (can do in next week)

Â  - Medium-term improvements (next month)

Â  - Long-term strategic changes (next quarter)

  

DELIVERABLES:

- Week 1 report: docs/retrospectives/week-1-production.md

- Optimization backlog (GitHub issues)

- Updated metrics dashboards

- Team learnings document

  

**Gather Data**:

  

  

# Export week 1 metrics

curl https://grafana.yourdomain.com/api/datasources/proxy/1/api/v1/query_range \

Â  -G --data-urlencode 'query=rate(hive_http_requests_total[1h])' \

Â  --data-urlencode 'start=2026-02-01T00:00:00Z' \

Â  --data-urlencode 'end=2026-02-08T00:00:00Z' \

Â  > week1-metrics.json

  

# Analyze costs

sqlite3 data/hive.db &lt;< 'EOF'

SELECTÂ 

Â  date(timestamp) as day,

Â  SUM(cost_usd) as daily_cost,

Â  COUNT(*) as requests,

Â  AVG(cost_usd) as avg_cost_per_request

FROM token_usage

WHERE timestamp >= datetime('now', '-7 days')

GROUP BY date(timestamp);

EOF

  

**Review with Team**:

  

  

# Week 1 Production Retrospective

  

## Metrics Summary

- **Total Requests**: 12,450

- **Total Cost**: $87.32 (under budget of $100/week)

- **Average Latency**: 234ms (p95: 890ms)

- **Error Rate**: 0.3% (target: &lt;1%)

- **Uptime**: 99.8% (downtime: 14 minutes on Day 3)

  

## What Went Well âœ…

- Launch was smooth, no major incidents

- Cost tracking helped identify expensive queries

- Cache hit rate of 42% saved $50+ in API costs

- Team responded quickly to Day 3 incident

  

## What Didn't Go Well âŒ

- Day 3 outage (database lock, 14 min downtime)

- Higher than expected token usage from Router

- Frontend bundle size larger than planned (350KB)

- One customer complaint about slow response

  

## Surprises ğŸ”

- Model routing saved more than expected (45% vs predicted 30%)

- Most traffic comes from 3 power users

- Semantic caching works better than keyword caching

  

## Optimization Plan

**Quick Wins** (this week):

- Optimize Router prompt (reduce tokens by 30%)

- Implement lazy loading for frontend charts

- Add database connection pooling

  

**Medium-Term** (this month):

- Migrate to PostgreSQL (resolve locking issues)

- Fine-tune cache TTLs based on actual data

- Add more aggressive rate limiting for outlier users

  

**Long-Term** (this quarter):

- Explore fine-tuned models (cheaper Router)

- Add read replicas for scaling

- Implement multi-region deployment

  

**âœ… Validation**:

â˜ Retrospective completed

â˜ Optimization backlog created

â˜ Team aligned on priorities

â˜ Monitoring refined based on learnings

**ğŸ“Š Log Cost**: ~$0.08 (25K tokens)

**âœ… Phase 8 Complete When:**

â˜ Production environment running

â˜ HTTPS working with valid certificate

â˜ Monitoring and alerting active

â˜ Security hardened

â˜ Runbook documented

â˜ Go-live checklist 100% complete

â˜ Week 1 retrospective done

â˜ System stable with &lt;1% error rate

â˜ Total cost &lt;$15 for Phase 8

**ğŸ’¡ Key Insight**: Production is not a destination, it's the beginning of continuous improvement.

**ğŸŠ CONGRATULATIONS! YOU'VE COMPLETED ALL 8 PHASES!**

**ğŸ† What You've Achieved**

**You now have**:

â€¢ âœ… Production-ready HIVE-R system with 70%+ test coverage

â€¢ âœ… Comprehensive cost monitoring and budget controls

â€¢ âœ… Full observability (logs, metrics, traces)

â€¢ âœ… Automated CI/CD pipeline

â€¢ âœ… Security hardened against common attacks

â€¢ âœ… Performance optimized (60-70% cost reduction)

â€¢ âœ… Professional deployment infrastructure

â€¢ âœ… Complete operational documentation

**Total Investment**:

â€¢ **Time**: 6 weeks (48-72 hours actual work)

â€¢ **AI Cost**: ~$200 in API calls

â€¢ **Infrastructure**: ~$50-100/month

â€¢ **Result**: Enterprise-grade multi-agent system

**ğŸ“Š FINAL VALIDATION CHECKLIST**

Use this to verify everything is working:

**Core Functionality**

â˜ Can invoke HIVE-R agents via MCP in Cursor

â˜ Can make requests via REST API

â˜ Agents coordinate correctly (Router â†’ Specialist)

â˜ Responses are streamed in real-time (SSE)

**Reliability**

â˜ System survives OpenAI API outage (circuit breaker + fallbacks)

â˜ Health checks detect real issues

â˜ Automatic recovery from transient failures

â˜ Backup and restore procedures tested

**Security**

â˜ No secrets in git repository

â˜ Input validation prevents injection attacks

â˜ Authentication enforced on protected routes

â˜ Rate limiting prevents abuse

â˜ Security scan shows 0 critical issues

**Performance**

â˜ Cache hit rate >40%

â˜ p95 latency &lt;1 second

â˜ Cost per request &lt;$0.05

â˜ Model routing working (cheap models for simple tasks)

**Observability**

â˜ Structured logs in JSON format

â˜ Prometheus metrics exposed

â˜ Grafana dashboards showing live data

â˜ Distributed tracing in Jaeger

â˜ Alerts firing correctly

**Operations**

â˜ CI runs tests on every PR

â˜ CD deploys to production on git tag

â˜ Monitoring active (UptimeRobot + Grafana)

â˜ Runbook documented and team trained

â˜ Incident response plan tested

**ğŸ“š