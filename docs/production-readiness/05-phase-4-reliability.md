---
title: "Phase 4: Reliability & Fallbacks"
sidebar_label: "Phase 4: Reliability & Fallbacks"
---

**

**â±ï¸ Time**: 8-10 hours

**ğŸ’° Budget**: $15-20

**ğŸ¯ Goal**: System survives LLM API outages

**ğŸ¤– AI Workers**: HIVE-R SRE + Builder agents

**ğŸ”„ Step 4.1: Implement Circuit Breaker**

**Your Instruction** (in Cursor):

  

  

Consult the SRE and Builder agents.

  

CONTEXT:

If OpenAI/Anthropic APIs go down, our entire system crashes. We need circuit breaker pattern.

  

TASK:

Implement circuit breaker for all LLM calls with these states:

Â  1. CLOSED (normal operation)

Â  2. OPEN (too many failures, stop calling API)

Â  3. HALF-OPEN (try one request to test if API recovered)

  

CONFIGURATION:

- Max failures: 3

- Timeout: 60 seconds (after 3 failures, wait 60s before retry)

- Track separately per model (OpenAI circuit != Anthropic circuit)

  

IMPLEMENTATION:

- New class: src/lib/circuit-breaker.ts

- Wrap all LLM calls (in middleware or agent wrapper)

- Emit events: circuit_opened, circuit_closed

- Log to monitoring system

  

REQUIREMENTS:

- Thread-safe (if we add workers later)

- Configurable via environment variables

- Graceful errors (user-friendly messages)

  

DELIVERABLES:

- CircuitBreaker class with tests

- Integration in src/middleware/llm-wrapper.ts

- Monitoring dashboard shows circuit state

- Documentation

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the circuit breaker logic:

â˜ States transition correctly (CLOSED â†’ OPEN â†’ HALF_OPEN â†’ CLOSED)

â˜ Timeouts are reasonable (60s, not 10 minutes)

â˜ Per-model tracking (OpenAI issues don't break Anthropic)

â˜ User gets friendly error: "AI service temporarily unavailable"

**Test manually**:

  

  

# Start server

npm run dev

  

# Simulate API failure (block network to api.openai.com)

sudo iptables -A OUTPUT -d api.openai.com -j DROP

  

# Make requests until circuit opens

curl http://localhost:3000/chat [repeat 3 times]

  

# Should get "Circuit breaker open" error

  

# Restore network

sudo iptables -D OUTPUT -d api.openai.com -j DROP

  

# Wait 60 seconds, try again

# Circuit should move to HALF_OPEN, then CLOSED

  

**âœ… Validation**:

â˜ Circuit opens after 3 failures

â˜ Circuit closes after successful recovery

â˜ Users get clear error messages

â˜ System doesn't crash during outage

**ğŸ“Š Log Cost**: ~$0.10 (30K tokens)

**ğŸ”€ Step 4.2: Add Model Fallbacks**

**Your Instruction** (in Cursor, open src/agents/router.ts):

  

  

Consult the SRE agent.

  

CONTEXT:

Router uses GPT-4o exclusively. If OpenAI is down, routing fails entirely.

  

TASK:

Implement fallback chain:

Â  1. Primary: GPT-4o with structured output

Â  2. Fallback 1: GPT-4o without structured output (parse text)

Â  3. Fallback 2: Claude 3.5 Sonnet

Â  4. Final fallback: Rule-based router (no LLM)

  

RULE-BASED LOGIC:

If all LLMs fail, use simple keyword matching:

Â  - "build", "code", "implement" â†’ builder

Â  - "design", "UI", "UX" â†’ designer

Â  - "security", "vulnerability" â†’ security

Â  - "deploy", "production" â†’ sre

Â  - Default â†’ product_manager (safe choice)

  

IMPLEMENTATION:

- Try each fallback in sequence

- Log which fallback was used (for monitoring)

- Track fallback usage (how often primary fails?)

  

DELIVERABLES:

- Enhanced router with fallback chain

- Rule-based fallback function

- Metrics: fallback_usage_count by level

- Tests for each fallback scenario

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the fallback logic:

â˜ Fallback order makes sense (cost-aware: expensive â†’ cheap â†’ free)

â˜ Rule-based router is sensible (covers common cases)

â˜ Each fallback is tested

â˜ Logs clearly show which fallback was used

**Test**:

  

  

# Mock failures to test fallbacks

# Use environment variable to force fallback mode

FORCE_FALLBACK_LEVEL=2 npm run dev

  

# Make request, check logs

curl http://localhost:3000/chat -d '&#123;"messages":[&#123;"role":"user","content":"build me a login page"&#125;]&#125;'

  

# Log should show: "Router fallback level 2 (Claude) used"

  

**âœ… Validation**:

â˜ Fallback chain works correctly

â˜ Rule-based router handles common cases

â˜ Performance acceptable (fallbacks don't add 10s delay)

â˜ Metrics track fallback usage

**ğŸ“Š Log Cost**: ~$0.12 (35K tokens)

**ğŸ” Step 4.3: Add Retry Logic with Exponential Backoff**

**Your Instruction** (in Cursor):

  

  

Consult the Builder agent.

  

CONTEXT:

Transient errors (rate limits, network hiccups) should be retried, not immediately failed.

  

TASK:

Implement retry logic for all LLM API calls:

Â  - Max retries: 3

Â  - Backoff strategy: exponential (1s, 2s, 4s)

Â  - Retry on: 429 (rate limit), 500 (server error), network timeout

Â  - Don't retry on: 400 (bad request), 401 (auth error)

  

INTEGRATION:

- Add to LLM wrapper middleware

- Works with circuit breaker (retries before opening circuit)

- Configurable via environment variables

  

REQUIREMENTS:

- Use existing retry library (p-retry or tenacity.js)

- Log each retry attempt

- Add jitter to prevent thundering herd

  

DELIVERABLES:

- Retry wrapper: src/lib/retry.ts

- Integration in LLM middleware

- Tests with mocked failures

- Documentation on retry behavior

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review retry configuration:

â˜ Max 3 retries (not infinite loop)

â˜ Exponential backoff (not fixed delay)

â˜ Jitter added (randomizes delay slightly)

â˜ Only retries transient errors (not auth failures)

**Test**:

  

  

# Mock transient failure (rate limit)

# Test that system retries 3 times, then fails

  

npm test tests/lib/retry.test.ts

  

# Check test includes:

# - Succeeds on 2nd attempt

# - Fails after 3 attempts

# - Respects backoff timing

  

**âœ… Validation**:

â˜ Retries transient errors

â˜ Respects max retry limit

â˜ Backoff timing correct

â˜ Doesn't retry non-transient errors

**ğŸ“Š Log Cost**: ~$0.08 (25K tokens)

**ğŸ¥ Step 4.4: Health Check System**

**Your Instruction** (in Cursor):

  

  

Consult the SRE agent.

  

CONTEXT:

We have /health/live and /health/ready endpoints but they're basic. Enhance them.

  

TASK:

Implement comprehensive health checks:

Â  - /health/live: Is process alive? (Always 200 unless crashed)

Â  - /health/ready: Is system ready to serve traffic?

Â  Â  - Database connection OK?

Â  Â  - LLM APIs reachable?

Â  Â  - Circuit breakers not all open?

Â  Â  - Disk space available?

Â  Â  - Memory usage < 90%?

  

RESPONSE FORMAT:

&#123;

Â  "status": "healthy" | "degraded" | "unhealthy",

Â  "checks": &#123;

Â  Â  "database": &#123;"status": "ok", "latency_ms": 5&#125;,

Â  Â  "openai_api": &#123;"status": "ok", "circuit": "closed"&#125;,

Â  Â  "anthropic_api": &#123;"status": "ok", "circuit": "closed"&#125;,

Â  Â  "disk_space": &#123;"status": "ok", "free_gb": 42&#125;,

Â  Â  "memory": &#123;"status": "ok", "usage_pct": 65&#125;

Â  &#125;,

Â  "timestamp": "2026-02-07T12:34:56Z"

&#125;

  

DELIVERABLES:

- Enhanced health check: src/routers/health.ts

- Health check functions: src/lib/health-checks.ts

- Tests for each check

- Kubernetes liveness/readiness probes configured

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review health check logic:

â˜ Checks are fast (&lt;500ms total)

â˜ Doesn't make expensive calls (test with cheap query)

â˜ Returns 503 (not 200) when unhealthy

â˜ Kubernetes probes use correct endpoints

**Test**:

  

  

# Healthy system

curl http://localhost:3000/health/ready

# Should return 200 with all checks "ok"

  

# Simulate unhealthy (stop database)

docker stop hive-postgres

  

curl http://localhost:3000/health/ready

# Should return 503 with database check "failed"

  

**âœ… Validation**:

â˜ Health checks detect real issues

â˜ Fast response (&lt;500ms)

â˜ Kubernetes probes configured

â˜ Monitoring system scrapes /health/ready

**ğŸ“Š Log Cost**: ~$0.08 (25K tokens)

**ğŸ“ Step 4.5: Document Failure Scenarios**

**Your Instruction** (in Cursor):

  

  

Consult the Tech Writer and SRE agents.

  

TASK:

Create runbook for common failure scenarios:

Â  1. OpenAI API down â†’ What happens? (circuit breaker, fallback to Claude)

Â  2. Database locked â†’ How to recover? (restart, check for long-running queries)

Â  3. Memory leak â†’ How to detect and mitigate? (health checks, auto-restart)

Â  4. Budget exceeded â†’ What to do? (alerts fired, manual intervention)

Â  5. Agent stuck in loop â†’ How to break? (timeout, manual kill)

  

FORMAT:

For each scenario:

Â  - Symptoms (how you'll know it's happening)

Â  - Detection (which monitoring alert fires)

Â  - Impact (what breaks for users)

Â  - Automatic mitigation (what system does)

Â  - Manual intervention (what human should do)

Â  - Prevention (how to avoid in future)

  

OUTPUT:

- docs/operations/runbook.md

- Link from README for operators

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the runbook:

â˜ Covers the 5 most likely failures

â˜ Actionable steps (not vague "investigate")

â˜ Includes commands to run (copy-pasteable)

â˜ Has escalation path (who to call if manual steps fail)

**âœ… Validation**:

â˜ Runbook exists and is comprehensive

â˜ You understand how to respond to each scenario

â˜ Commands tested (actually work)

**ğŸ“Š Log Cost**: ~$0.05 (15K tokens)

**âœ… Phase 4 Complete When:**

â˜ Circuit breaker implemented and tested

â˜ Model fallbacks working (4-level chain)

â˜ Retry logic active

â˜ Health checks comprehensive

â˜ Runbook documented

â˜ System survives simulated OpenAI outage

â˜ Total cost &lt;$20 for Phase 4

**ğŸ’¡ Key Insight**: Your system can now withstand LLM API outages without crashing.

**ğŸ HIVE-R Production Readiness Guide (CONTINUED)**

**