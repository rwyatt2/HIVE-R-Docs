---
title: "Phase 5: Observability Stack"
sidebar_label: "Phase 5: Observability Stack"
---

**

**â±ï¸ Time**: 10-12 hours

**ðŸ’° Budget**: $20-25

**ðŸŽ¯ Goal**: Deep visibility into system behavior

**ðŸ¤– AI Workers**: HIVE-R SRE + Data Analyst + Builder agents

**ðŸ“Š Step 5.1: Implement Structured Logging**

**Your Instruction** (in Cursor):

  

  

Consult the SRE and Builder agents.

  

CONTEXT:

We're using console.log() scattered everywhere. This is unmaintainable in production.

  

TASK:

Migrate to structured JSON logging with Pino:

Â  1. Replace all console.log/error/warn with logger

Â  2. Add context to every log (requestId, userId, agentName)

Â  3. Configure log levels by environment (debug in dev, info in prod)

Â  4. Add log rotation (don't fill disk)

Â  5. Format for log aggregation tools (ELK, Datadog)

  

LOG STRUCTURE:

&#123;

Â  "level": "info",

Â  "time": "2026-02-07T12:34:56.789Z",

Â  "requestId": "req-123",

Â  "userId": "user-456",

Â  "agentName": "builder",

Â  "msg": "Agent invocation started",

Â  "duration": 1234,

Â  "tokens": 500,

Â  "cost": 0.015

&#125;

  

IMPLEMENTATION:

- Install: pino, pino-pretty (dev), pino-rotating-file-stream

- Create logger singleton: src/lib/logger.ts

- Add middleware to inject requestId

- Update ALL files that use console.log

  

DELIVERABLES:

- Logger configuration

- Migration of all console.log calls

- Log rotation setup (daily, keep 7 days)

- Environment-based log levels

- Documentation: docs/operations/logging.md

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the logger configuration:

â˜ Uses JSON in production (for parsing)

â˜ Uses pretty-print in development (for readability)

â˜ RequestId propagates through all logs in a request

â˜ Sensitive data not logged (API keys, passwords)

**Test**:

  

  

# Start server, make request

npm run dev

curl http://localhost:3000/chat -d '&#123;"messages":[&#123;"role":"user","content":"test"&#125;]&#125;'

  

# Check logs (should be JSON in one line)

tail -f logs/hive.log | jq

  

# Should see structured logs with all context fields

  

**âœ… Validation**:

â˜ All console.logs replaced

â˜ Logs are structured JSON

â˜ RequestId tracks through multi-agent flow

â˜ Log rotation working (check after 24h)

â˜ No sensitive data in logs

**ðŸ“Š Log Cost**: ~$0.20 (60K tokens - lots of file changes)

**ðŸ“ˆ Step 5.2: Add Prometheus Metrics**

**Your Instruction** (in Cursor):

  

  

Consult the SRE agent.

  

CONTEXT:

Logs tell us what happened. Metrics tell us trends over time.

  

TASK:

Implement Prometheus metrics for:

Â  - Request rate (requests/sec by endpoint)

Â  - Response latency (histogram: p50, p95, p99)

Â  - Error rate (% of requests that failed)

Â  - Agent invocations (counter by agent name)

Â  - Token usage (counter by model)

Â  - Cost (gauge: current spend rate)

Â  - Circuit breaker state (gauge: 0=closed, 1=open)

  

IMPLEMENTATION:

- Install: prom-client

- Create metrics registry: src/lib/metrics.ts

- Add middleware to track HTTP metrics

- Expose /metrics endpoint (for Prometheus scraper)

- Custom metrics for agent-specific tracking

  

METRIC NAMING:

Follow Prometheus conventions:

- hive_http_requests_total (counter)

- hive_http_request_duration_seconds (histogram)

- hive_agent_invocations_total (counter)

- hive_token_usage_total (counter)

- hive_cost_dollars (gauge)

  

DELIVERABLES:

- Metrics registry and instrumentation

- /metrics endpoint

- Grafana dashboard JSON (for import)

- Documentation on adding custom metrics

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the metrics design:

â˜ Covers the key system behaviors

â˜ Has labels for filtering (agent, model, status_code)

â˜ Histogram buckets appropriate (0.1s, 0.5s, 1s, 5s, 10s)

â˜ /metrics endpoint doesn't require auth (Prometheus needs it)

**Test**:

  

  

# Start server

npm run dev

  

# Make some requests

for i in &#123;1..10&#125;; do

Â  curl http://localhost:3000/chat -d '&#123;"messages":[&#123;"role":"user","content":"test"&#125;]&#125;'

done

  

# Check metrics endpoint

curl http://localhost:3000/metrics

  

# Should see Prometheus format:

# hive_http_requests_total&#123;method="POST",path="/chat",status="200"&#125; 10

# hive_agent_invocations_total&#123;agent="router"&#125; 10

  

**âœ… Validation**:

â˜ /metrics endpoint returns valid Prometheus format

â˜ Metrics update in real-time

â˜ Includes both system and custom metrics

â˜ No performance impact (&lt;5ms per request)

**ðŸ“Š Log Cost**: ~$0.12 (35K tokens)

**ðŸ“‰ Step 5.3: Set Up Grafana Dashboard**

**Your Instruction** (in Cursor):

  

  

Consult the Data Analyst and SRE agents.

  

CONTEXT:

We have metrics. Now we need visualization.

  

TASK:

Data Analyst: Design dashboard layout with panels for:

Â  - Request rate (line chart, last 1h)

Â  - Response latency (heatmap, p50/p95/p99)

Â  - Error rate (gauge, % of requests)

Â  - Agent usage (bar chart, invocations per agent)

Â  - Token usage (stacked area, by model over time)

Â  - Cost burn rate (line chart, $/hour)

Â  - Circuit breaker status (status indicator)

  

SRE: Create Grafana dashboard JSON:

Â  - Use Prometheus as data source

Â  - Auto-refresh every 30s

Â  - Time range selector (1h, 6h, 24h, 7d)

Â  - Variables for filtering (agent, model)

Â  - Alert rules (error rate >5%, cost >$10/hour)

  

DELIVERABLES:

- Grafana dashboard JSON: grafana/dashboards/hive-overview.json

- Docker compose with Grafana + Prometheus

- Setup instructions: docs/operations/monitoring-setup.md

- Screenshot of dashboard for README

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the dashboard design (Data Analyst will provide mockup):

â˜ Most important metrics above the fold

â˜ Color coding (green=good, yellow=warning, red=critical)

â˜ Time range covers operational needs (1h for incidents, 7d for trends)

**Setup Grafana**:

  

  

# Add to docker-compose.yml

cat >> docker-compose.yml &lt;< 'EOF'

Â  prometheus:

Â  Â  image: prom/prometheus:latest

Â  Â  volumes:

Â  Â  Â  - ./prometheus.yml:/etc/prometheus/prometheus.yml

Â  Â  Â  - prometheus-data:/prometheus

Â  Â  ports:

Â  Â  Â  - "9090:9090"

Â  grafana:

Â  Â  image: grafana/grafana:latest

Â  Â  volumes:

Â  Â  Â  - ./grafana/dashboards:/etc/grafana/provisioning/dashboards

Â  Â  Â  - ./grafana/datasources:/etc/grafana/provisioning/datasources

Â  Â  Â  - grafana-data:/var/lib/grafana

Â  Â  ports:

Â  Â  Â  - "3001:3000"

Â  Â  environment:

Â  Â  Â  - GF_SECURITY_ADMIN_PASSWORD=admin

EOF

  

# Create Prometheus config

cat > prometheus.yml &lt;< 'EOF'

global:

Â  scrape_interval: 15s

  

scrape_configs:

Â  - job_name: 'hive'

Â  Â  static_configs:

Â  Â  Â  - targets: ['host.docker.internal:3000']

EOF

  

# Start monitoring stack

docker-compose up -d prometheus grafana

  

**Access Grafana**:

1. Open [http://localhost:3001](http://localhost:3001)

2. Login (admin/admin)

3. Add Prometheus data source ([http://prometheus:9090](http://prometheus:9090))

4. Import dashboard JSON

**âœ… Validation**:

â˜ Grafana loads dashboard

â˜ All panels show data

â˜ Metrics update in real-time

â˜ Alerts configured

**ðŸ“Š Log Cost**: ~$0.10 (30K tokens)

**ðŸ” Step 5.4: Implement Distributed Tracing**

**Your Instruction** (in Cursor):

  

  

Consult the SRE agent.

  

CONTEXT:

When a request flows through 5+ agents, we need to trace it end-to-end.

  

TASK:

Implement OpenTelemetry distributed tracing:

Â  - Trace a request from entry (/chat) through all agent invocations

Â  - Each agent span shows: name, duration, model used, tokens, cost

Â  - Parent-child relationships visible (Router â†’ Builder â†’ Reviewer)

Â  - Export traces to Jaeger (visualization)

  

IMPLEMENTATION:

- Install: @opentelemetry/api, @opentelemetry/sdk-node, @opentelemetry/exporter-jaeger

- Initialize tracer: src/lib/tracer.ts

- Instrument HTTP server (auto)

- Manually instrument agent invocations

- Propagate trace context through LangGraph state

  

SPAN ATTRIBUTES:

- agent.name

- agent.model

- agent.tokens_in

- agent.tokens_out

- agent.cost

- agent.duration_ms

  

DELIVERABLES:

- Tracer initialization

- Agent instrumentation

- Jaeger setup in docker-compose

- Documentation: docs/operations/tracing.md

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review the tracing approach:

â˜ Spans have meaningful names (not "function1", "function2")

â˜ Trace context propagates through async agent calls

â˜ Doesn't add >10ms latency per request

â˜ Sampling configured (don't trace 100% in prod, maybe 10%)

**Setup Jaeger**:

  

  

# Add to docker-compose.yml

cat >> docker-compose.yml &lt;< 'EOF'

Â  jaeger:

Â  Â  image: jaegertracing/all-in-one:latest

Â  Â  ports:

Â  Â  Â  - "16686:16686"Â  # UI

Â  Â  Â  - "14268:14268"Â  # Collector

Â  Â  environment:

Â  Â  Â  - COLLECTOR_ZIPKIN_HTTP_PORT=9411

EOF

  

docker-compose up -d jaeger

  

**Test**:

  

  

# Make a request

curl http://localhost:3000/chat -d '&#123;"messages":[&#123;"role":"user","content":"build a button"&#125;]&#125;'

  

# Open Jaeger UI

open http://localhost:16686

  

# Search for traces, select one

# Should see waterfall: Router â†’ Builder â†’ (other agents)

  

**âœ… Validation**:

â˜ Traces appear in Jaeger

â˜ Spans show parent-child relationships

â˜ Agent names and durations visible

â˜ Can trace a request through 5+ agents

**ðŸ“Š Log Cost**: ~$0.15 (45K tokens)

**ðŸš¨ Step 5.5: Configure Alerting**

**Your Instruction** (in Cursor):

  

  

Consult the SRE agent.

  

CONTEXT:

Metrics and dashboards are reactive. We need proactive alerts.

  

TASK:

Configure Prometheus alerting rules for:

Â  - High error rate: >5% of requests failing (5min average)

Â  - High latency: p95 >5 seconds (5min average)

Â  - Agent failures: Any agent failing >50% of time

Â  - Cost spike: $/hour >$20 (when monthly budget is $1500)

Â  - Circuit breaker open: Any circuit open >5 minutes

Â  - Low disk space: &lt;10GB remaining

  

ALERT ROUTING:

- Critical: Page on-call (PagerDuty/Opsgenie)

- Warning: Slack notification

- Info: Log only

  

IMPLEMENTATION:

- Prometheus alert rules: prometheus/alerts.yml

- Alertmanager config: prometheus/alertmanager.yml

- Slack webhook integration

- Test alerts with mock failures

  

DELIVERABLES:

- Alert rules file

- Alertmanager configuration

- Docker compose with Alertmanager

- Runbook links in alerts (so responder knows what to do)

  

**â¸ï¸ YOUR APPROVAL GATE**:

Review alert thresholds:

â˜ Not too sensitive (won't page for every hiccup)

â˜ Not too lenient (catches real issues)

â˜ Has runbook links (alerts are actionable)

â˜ Grouped to prevent alert storm (max 1 alert per 5min)

**Setup Alertmanager**:

  

  

# Create alert rules

cat > prometheus/alerts.yml &lt;< 'EOF'

groups:

Â  - name: hive_alerts

Â  Â  interval: 30s

Â  Â  rules:

Â  Â  Â  - alert: HighErrorRate

Â  Â  Â  Â  expr: rate(hive_http_requests_total&#123;status=~"5.."&#125;[5m]) > 0.05

Â  Â  Â  Â  for: 5m

Â  Â  Â  Â  labels:

Â  Â  Â  Â  Â  severity: critical

Â  Â  Â  Â  annotations:

Â  Â  Â  Â  Â  summary: "High error rate: &#123;&#123; $value &#125;&#125;%"

Â  Â  Â  Â  Â  runbook: "https://github.com/you/hive-r/docs/runbook.md#high-error-rate"

Â  Â  Â  - alert: HighCostBurn

Â  Â  Â  Â  expr: rate(hive_cost_dollars[1h]) * 3600 > 20

Â  Â  Â  Â  for: 10m

Â  Â  Â  Â  labels:

Â  Â  Â  Â  Â  severity: warning

Â  Â  Â  Â  annotations:

Â  Â  Â  Â  Â  summary: "Cost burn rate: $&#123;&#123; $value &#125;&#125;/hour"

EOF

  

# Add Alertmanager to docker-compose.yml

cat >> docker-compose.yml &lt;< 'EOF'

Â  alertmanager:

Â  Â  image: prom/alertmanager:latest

Â  Â  volumes:

Â  Â  Â  - ./prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml

Â  Â  ports:

Â  Â  Â  - "9093:9093"

EOF

  

**Test alerts**:

  

  

# Trigger high error rate (stop database to cause errors)

docker stop hive-postgres

  

# Make requests (should fail)

for i in &#123;1..20&#125;; do curl http://localhost:3000/chat; done

  

# Wait 5 minutes

# Check Alertmanager: http://localhost:9093

# Should see "HighErrorRate" alert firing

  

**âœ… Validation**:

â˜ Alerts fire correctly

â˜ Slack notifications work

â˜ Alerts resolve when issue fixed

â˜ No alert spam (grouped properly)

**ðŸ“Š Log Cost**: ~$0.08 (25K tokens)

**âœ… Phase 5 Complete When:**

â˜ Structured logging active (all console.logs replaced)

â˜ Prometheus metrics exposed

â˜ Grafana dashboard showing real data

â˜ Distributed tracing working (Jaeger shows spans)

â˜ Alerting configured and tested

â˜ Monitoring stack runs in Docker

â˜ Total cost &lt;$25 for Phase 5

**ðŸ’¡ Key Insight**: You can now debug production issues 10x faster with full observability.

**ðŸ“¸ Take Screenshot**: Of Grafana dashboard + Jaeger trace for your portfolio/docs.

**